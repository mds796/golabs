From bd9f723452a3b47b869307a5117fb33098838694 Mon Sep 17 00:00:00 2001
From: Miguel Salcedo <msalcedo92@gmail.com>
Date: Sat, 20 Oct 2018 16:39:59 +0000
Subject: [PATCH] lab2

---
 src/.gitignore                        |   1 +
 src/simplepb/backup.go                | 108 +++++
 src/simplepb/mvp307_mds796_lab2.patch | 754 ++++++++++++++++++++++++++++++++++
 src/simplepb/primary.go               |  69 ++++
 src/simplepb/priority_queue.go        |  45 ++
 src/simplepb/recover.go               |  90 ++++
 src/simplepb/server.go                | 239 ++++++++---
 7 files changed, 1243 insertions(+), 63 deletions(-)
 create mode 100644 src/simplepb/backup.go
 create mode 100644 src/simplepb/mvp307_mds796_lab2.patch
 create mode 100644 src/simplepb/primary.go
 create mode 100644 src/simplepb/priority_queue.go
 create mode 100644 src/simplepb/recover.go

diff --git a/src/.gitignore b/src/.gitignore
index 88fe0eb..b94896c 100644
--- a/src/.gitignore
+++ b/src/.gitignore
@@ -5,3 +5,4 @@ mrtmp.*
 /mapreduce/x.txt
 /pbservice/x.txt
 /kvpaxos/x.txt
+.idea
\ No newline at end of file
diff --git a/src/simplepb/backup.go b/src/simplepb/backup.go
new file mode 100644
index 0000000..b39faeb
--- /dev/null
+++ b/src/simplepb/backup.go
@@ -0,0 +1,108 @@
+package simplepb
+
+import (
+	"container/heap"
+	"log"
+	"time"
+)
+
+// Process prepare messages in the backup, doing state transfer if necessary.
+// Enqueues messages received out of order to process them in order.
+// However, the RPC may still return a response without processing the operation.
+// This case is alright since the master commits all previous operations if any future operations return true.
+// So, once we have all in order operations the next operation will eventually return true.
+func (srv *PBServer) backupPrepare(arguments *PrepareArgs, reply *PrepareReply) {
+	srv.prepareReply(arguments, reply)
+
+	if arguments.View > srv.currentView || (reply.Success && arguments.Index > srv.opIndex) {
+		// Add the prepared operation to the min-heap for uncommitted ops
+		prepare := &Prepare{args: arguments, reply: reply, done: make(chan bool, 1)}
+		go srv.backupPrepareInOrder(prepare)
+		<-prepare.done
+	}
+}
+
+func (srv *PBServer) prepareReply(arguments *PrepareArgs, reply *PrepareReply) {
+	log.Printf("Node %v - [view %v commit %v op %v] - Received prepare (view: %v op: %v commit: %v entry: %v)", srv.me, srv.currentView, srv.commitIndex, srv.opIndex, srv.currentView, arguments.Index, arguments.PrimaryCommit, arguments.Entry)
+
+	reply.View = srv.currentView
+	reply.Success = srv.status == NORMAL && !srv.IsPrimary() && arguments.View >= srv.currentView
+
+	if arguments.PrimaryCommit > srv.commitIndex {
+		srv.mu.Lock()
+		defer srv.mu.Unlock()
+
+		srv.commitIndex = arguments.PrimaryCommit
+		srv.timeLastCommit = time.Now()
+	}
+}
+
+// Prepares the next set of operations that are in order from the current opIndex
+// Does nothing if the next operation is not in the PrepareQueue.
+// For example, if we had operations [2, 4, 5] and we just received a request to prepare #1,
+// We would process ops 1 and 2, but not 4 and 5. Once we receive the message to prepare 3,
+// only then would we prepare 3, 4, and 5 in order.
+// Notice that this may cause the primary to timeout waiting for a response, but that's okay.
+func (srv *PBServer) backupPrepareInOrder(prepare *Prepare) {
+	srv.mu.Lock()
+	defer srv.mu.Unlock()
+
+	timer := time.AfterFunc(srv.noCommitThreshold, func() { srv.backupRecover(prepare) })
+
+	// Only go into recovery if it wasn't in recovery yet
+	// Also, verify the time of the last commit update, since a timer might have been set
+	// right after recovery finished
+	if prepare.args.View > srv.currentView {
+		go srv.backupRecover(prepare)
+		return
+	}
+
+	log.Printf("Replica %v - Preparing (view: %v op: %v commit: %v entry: %v)", srv.me, prepare.args.View, prepare.args.Index, prepare.args.PrimaryCommit, prepare.args.Entry)
+
+	heap.Push(&srv.uncommittedOperations, prepare)
+	srv.executeUncommittedOperations()
+
+	// Disable recovery timer if we managed to process all operations in the queue
+	if srv.uncommittedOperations.Len() == 0 {
+		timer.Stop()
+	}
+}
+
+func (srv *PBServer) backupRecover(prepare *Prepare) {
+	if prepare.args.View <= srv.currentView && time.Since(srv.timeLastCommit).Nanoseconds() < srv.noCommitThreshold.Nanoseconds() {
+		return
+	}
+
+	srv.recover()
+	srv.backupPrepareInOrder(prepare)
+}
+
+func (srv *PBServer) executeUncommittedOperations() {
+	// If the next operation in the uncommitted ops queue is the next to be committed,
+	// then commit the operations until you reach an operation that is out of order
+	//
+	// Otherwise, it waits for the next operations. If the next operation finally arrives,
+	// we process it and all the following ones.
+	//
+	// In case the next operation does not arrive in a timely fashion, we consider
+	// the message to be lost. In this case, the recovery timer will fire and execute
+	// the recovery protocol.
+	for srv.uncommittedOperations.Len() > 0 && srv.isNextOperation(srv.uncommittedOperations.Peek()) {
+		message := heap.Pop(&srv.uncommittedOperations).(*Prepare)
+
+		if message.args.Index > srv.opIndex {
+			srv.opIndex = message.args.Index
+			srv.log = append(srv.log, message.args.Entry)
+		}
+
+		message.reply.Success = srv.currentView == message.args.View && srv.opIndex >= message.args.Index
+		message.reply.View = srv.currentView
+		message.done <- true
+
+		log.Printf("Replica %v - Prepared (view: %v op: %v commit: %v entry: %v)", srv.me, message.args.View, message.args.Index, message.args.PrimaryCommit, message.args.Entry)
+	}
+}
+
+func (srv *PBServer) isNextOperation(prepare *Prepare) bool {
+	return prepare.args.Index <= (srv.opIndex + 1)
+}
diff --git a/src/simplepb/mvp307_mds796_lab2.patch b/src/simplepb/mvp307_mds796_lab2.patch
new file mode 100644
index 0000000..c985d23
--- /dev/null
+++ b/src/simplepb/mvp307_mds796_lab2.patch
@@ -0,0 +1,754 @@
+From ba04f2fa6d589251a5d1829ab38e69058bfc02c7 Mon Sep 17 00:00:00 2001
+From: Miguel Salcedo <msalcedo92@gmail.com>
+Date: Sat, 20 Oct 2018 09:00:14 +0000
+Subject: [PATCH] Implement lab 1 to pass tests 1A
+
+---
+ src/simplepb/backup.go         | 108 +++++++++++++++++++
+ src/simplepb/primary.go        |  69 ++++++++++++
+ src/simplepb/priority_queue.go |  45 ++++++++
+ src/simplepb/recover.go        |  86 +++++++++++++++
+ src/simplepb/server.go         | 239 ++++++++++++++++++++++++++++++-----------
+ 5 files changed, 484 insertions(+), 63 deletions(-)
+ create mode 100644 src/simplepb/backup.go
+ create mode 100644 src/simplepb/primary.go
+ create mode 100644 src/simplepb/priority_queue.go
+ create mode 100644 src/simplepb/recover.go
+
+diff --git a/src/simplepb/backup.go b/src/simplepb/backup.go
+new file mode 100644
+index 0000000..b39faeb
+--- /dev/null
++++ b/src/simplepb/backup.go
+@@ -0,0 +1,108 @@
++package simplepb
++
++import (
++	"container/heap"
++	"log"
++	"time"
++)
++
++// Process prepare messages in the backup, doing state transfer if necessary.
++// Enqueues messages received out of order to process them in order.
++// However, the RPC may still return a response without processing the operation.
++// This case is alright since the master commits all previous operations if any future operations return true.
++// So, once we have all in order operations the next operation will eventually return true.
++func (srv *PBServer) backupPrepare(arguments *PrepareArgs, reply *PrepareReply) {
++	srv.prepareReply(arguments, reply)
++
++	if arguments.View > srv.currentView || (reply.Success && arguments.Index > srv.opIndex) {
++		// Add the prepared operation to the min-heap for uncommitted ops
++		prepare := &Prepare{args: arguments, reply: reply, done: make(chan bool, 1)}
++		go srv.backupPrepareInOrder(prepare)
++		<-prepare.done
++	}
++}
++
++func (srv *PBServer) prepareReply(arguments *PrepareArgs, reply *PrepareReply) {
++	log.Printf("Node %v - [view %v commit %v op %v] - Received prepare (view: %v op: %v commit: %v entry: %v)", srv.me, srv.currentView, srv.commitIndex, srv.opIndex, srv.currentView, arguments.Index, arguments.PrimaryCommit, arguments.Entry)
++
++	reply.View = srv.currentView
++	reply.Success = srv.status == NORMAL && !srv.IsPrimary() && arguments.View >= srv.currentView
++
++	if arguments.PrimaryCommit > srv.commitIndex {
++		srv.mu.Lock()
++		defer srv.mu.Unlock()
++
++		srv.commitIndex = arguments.PrimaryCommit
++		srv.timeLastCommit = time.Now()
++	}
++}
++
++// Prepares the next set of operations that are in order from the current opIndex
++// Does nothing if the next operation is not in the PrepareQueue.
++// For example, if we had operations [2, 4, 5] and we just received a request to prepare #1,
++// We would process ops 1 and 2, but not 4 and 5. Once we receive the message to prepare 3,
++// only then would we prepare 3, 4, and 5 in order.
++// Notice that this may cause the primary to timeout waiting for a response, but that's okay.
++func (srv *PBServer) backupPrepareInOrder(prepare *Prepare) {
++	srv.mu.Lock()
++	defer srv.mu.Unlock()
++
++	timer := time.AfterFunc(srv.noCommitThreshold, func() { srv.backupRecover(prepare) })
++
++	// Only go into recovery if it wasn't in recovery yet
++	// Also, verify the time of the last commit update, since a timer might have been set
++	// right after recovery finished
++	if prepare.args.View > srv.currentView {
++		go srv.backupRecover(prepare)
++		return
++	}
++
++	log.Printf("Replica %v - Preparing (view: %v op: %v commit: %v entry: %v)", srv.me, prepare.args.View, prepare.args.Index, prepare.args.PrimaryCommit, prepare.args.Entry)
++
++	heap.Push(&srv.uncommittedOperations, prepare)
++	srv.executeUncommittedOperations()
++
++	// Disable recovery timer if we managed to process all operations in the queue
++	if srv.uncommittedOperations.Len() == 0 {
++		timer.Stop()
++	}
++}
++
++func (srv *PBServer) backupRecover(prepare *Prepare) {
++	if prepare.args.View <= srv.currentView && time.Since(srv.timeLastCommit).Nanoseconds() < srv.noCommitThreshold.Nanoseconds() {
++		return
++	}
++
++	srv.recover()
++	srv.backupPrepareInOrder(prepare)
++}
++
++func (srv *PBServer) executeUncommittedOperations() {
++	// If the next operation in the uncommitted ops queue is the next to be committed,
++	// then commit the operations until you reach an operation that is out of order
++	//
++	// Otherwise, it waits for the next operations. If the next operation finally arrives,
++	// we process it and all the following ones.
++	//
++	// In case the next operation does not arrive in a timely fashion, we consider
++	// the message to be lost. In this case, the recovery timer will fire and execute
++	// the recovery protocol.
++	for srv.uncommittedOperations.Len() > 0 && srv.isNextOperation(srv.uncommittedOperations.Peek()) {
++		message := heap.Pop(&srv.uncommittedOperations).(*Prepare)
++
++		if message.args.Index > srv.opIndex {
++			srv.opIndex = message.args.Index
++			srv.log = append(srv.log, message.args.Entry)
++		}
++
++		message.reply.Success = srv.currentView == message.args.View && srv.opIndex >= message.args.Index
++		message.reply.View = srv.currentView
++		message.done <- true
++
++		log.Printf("Replica %v - Prepared (view: %v op: %v commit: %v entry: %v)", srv.me, message.args.View, message.args.Index, message.args.PrimaryCommit, message.args.Entry)
++	}
++}
++
++func (srv *PBServer) isNextOperation(prepare *Prepare) bool {
++	return prepare.args.Index <= (srv.opIndex + 1)
++}
+diff --git a/src/simplepb/primary.go b/src/simplepb/primary.go
+new file mode 100644
+index 0000000..91b4c06
+--- /dev/null
++++ b/src/simplepb/primary.go
+@@ -0,0 +1,69 @@
++package simplepb
++
++import (
++	"log"
++)
++
++func (srv *PBServer) isNormalPrimary() bool {
++	return srv.status == NORMAL && srv.IsPrimary()
++}
++
++func (srv *PBServer) primaryPrepare(opIndex int) {
++	// Normally, we would update the client table with the new request number before sending Prepare messages.
++	arguments := &PrepareArgs{View: srv.currentView, PrimaryCommit: srv.commitIndex, Index: opIndex, Entry: srv.log[opIndex]}
++
++	log.Printf("Primary %v - Preparing (view: %v op: %v commit: %v entry: %v)", srv.me, srv.currentView, arguments.Index, srv.commitIndex, arguments.Entry)
++
++	replies := make(chan *PrepareReply, len(srv.peers))
++
++	for peer := range srv.peers {
++		if peer != srv.me {
++			go srv.primarySendPrepare(peer, arguments, replies)
++		}
++	}
++
++	go srv.primaryAwaitPrepare(arguments, replies)
++}
++
++func (srv *PBServer) primarySendPrepare(peer int, arguments *PrepareArgs, replies chan *PrepareReply) {
++	reply := new(PrepareReply)
++	completed := srv.sendPrepare(peer, arguments, reply)
++
++	if !completed {
++		replies <- nil
++	} else {
++		replies <- reply
++	}
++}
++
++// Awaits all prepare responses and timeouts.
++// Then, counts the number of successful replies. If >= f replies were successful, appends the next operation to the log.
++func (srv *PBServer) primaryAwaitPrepare(arguments *PrepareArgs, replies chan *PrepareReply) {
++	majority := srv.replicationFactor()
++	success := 0
++	failure := 0
++
++	// index starts at 1 in order to skip the primary.
++	// stops immediately after f successes
++	for i := 1; success < majority && i < len(srv.peers); i++ {
++		reply := <-replies
++
++		if reply == nil || !reply.Success {
++			failure++
++		} else {
++			success++
++		}
++	}
++
++	if success >= majority {
++		srv.mu.Lock()
++		defer srv.mu.Unlock()
++
++		if srv.commitIndex < arguments.Index && srv.isNormalPrimary() {
++			log.Printf("Primary %v - Updating commit %v -> %v", srv.me, srv.commitIndex, arguments.Index)
++			srv.commitIndex = arguments.Index
++		}
++	} else if !srv.isNormalPrimary() {
++		log.Printf("Primary %v - Failed serving operation %v", srv.me, arguments.Index)
++	}
++}
+diff --git a/src/simplepb/priority_queue.go b/src/simplepb/priority_queue.go
+new file mode 100644
+index 0000000..d7a4200
+--- /dev/null
++++ b/src/simplepb/priority_queue.go
+@@ -0,0 +1,45 @@
++package simplepb
++
++// Prepare is a pair of input and output used by the backup
++type Prepare struct {
++	args  *PrepareArgs
++	reply *PrepareReply
++	done  chan bool
++}
++
++// OperationsQueue is a PriorityQueue that implements heap.Interface and holds *Prepare.
++type OperationsQueue []*Prepare
++
++// The length of the queue
++func (pq OperationsQueue) Len() int {
++	return len(pq)
++}
++
++// True if i is less than j
++func (pq OperationsQueue) Less(i, j int) bool {
++	return pq[i].args.Index < pq[j].args.Index
++}
++
++// Swap the positions of i and j
++func (pq OperationsQueue) Swap(i, j int) {
++	pq[i], pq[j] = pq[j], pq[i]
++}
++
++// Push x unto the queue
++func (pq *OperationsQueue) Push(x interface{}) {
++	*pq = append(*pq, x.(*Prepare))
++}
++
++// Pop the next operation to be committed from the queue
++func (pq *OperationsQueue) Pop() interface{} {
++	old := *pq
++	n := len(old)
++	item := old[n-1]
++	*pq = old[0 : n-1]
++	return item
++}
++
++// Peek at the next operation to be committed
++func (pq *OperationsQueue) Peek() *Prepare {
++	return (*pq)[0]
++}
+diff --git a/src/simplepb/recover.go b/src/simplepb/recover.go
+new file mode 100644
+index 0000000..1c4d6c2
+--- /dev/null
++++ b/src/simplepb/recover.go
+@@ -0,0 +1,86 @@
++package simplepb
++
++import (
++	"log"
++	"time"
++)
++
++func (srv *PBServer) recover() {
++	arguments, replies := srv.startRecovery()
++	srv.awaitRecovery(arguments, replies)
++}
++
++func (srv *PBServer) startRecovery() (arguments *RecoveryArgs, replies chan *RecoveryReply) {
++	srv.mu.Lock()
++	defer srv.mu.Unlock()
++
++	if srv.status != NORMAL {
++		log.Printf("Node %v - not in normal status (view: %v op: %v commit: %v, status: %d)", srv.me, srv.currentView, srv.opIndex, srv.commitIndex, srv.status)
++		return
++	}
++
++	log.Printf("Node %v - Recovering (view: %v op: %v commit: %v)", srv.me, srv.currentView, srv.opIndex, srv.commitIndex)
++
++	srv.status = RECOVERING
++
++	arguments = &RecoveryArgs{View: srv.currentView, Server: srv.me}
++	replies = make(chan *RecoveryReply, len(srv.peers))
++
++	// Send recovery requests to all peers. However, only the primary will reply with success
++	for peer := range srv.peers {
++		if peer != srv.me {
++			go srv.sendRecoveryToPeer(peer, arguments, replies)
++		}
++	}
++
++	return arguments, replies
++}
++
++func (srv *PBServer) sendRecoveryToPeer(peer int, arguments *RecoveryArgs, replies chan *RecoveryReply) {
++	reply := new(RecoveryReply)
++	completed := srv.sendRecovery(peer, arguments, reply)
++
++	if !completed {
++		replies <- nil
++	} else {
++		replies <- reply
++	}
++}
++
++// Update log, op index, commit index and server status on receiving one recovery reply
++func (srv *PBServer) awaitRecovery(arguments *RecoveryArgs, replies chan *RecoveryReply) {
++	var primary *RecoveryReply
++	success := 0
++
++	// index starts at 1 in order to skip the current server.
++	for i := 1; i < len(srv.peers); i++ {
++		reply := <-replies
++
++		if reply != nil {
++			success++
++
++			if reply.Success {
++				primary = reply
++			}
++		}
++	}
++
++	if success >= srv.replicationFactor() && primary != nil {
++		srv.mu.Lock()
++		defer srv.mu.Unlock()
++
++		for i := len(srv.log); i < len(primary.Entries); i++ {
++			srv.appendCommand(primary.Entries[i])
++		}
++
++		srv.currentView = primary.View
++		srv.commitIndex = primary.PrimaryCommit
++		srv.timeLastCommit = time.Now()
++		srv.status = NORMAL
++		srv.lastNormalView = srv.currentView
++
++		log.Printf("Node %v - recovered with commit index %d and op index %d.\n", srv.me, srv.commitIndex, srv.opIndex)
++	} else {
++		log.Printf("Node %v - Did not received sufficient recovery replies (%d) or primary did not reply (%v)", srv.me, success, primary == nil)
++	}
++}
+diff --git a/src/simplepb/server.go b/src/simplepb/server.go
+index ca5efb4..9c6b11d 100644
+--- a/src/simplepb/server.go
++++ b/src/simplepb/server.go
+@@ -1,3 +1,7 @@
++// Authors:
++// Miguel David Salcedo   - NetID: mds796
++// Matheus Vieira Portela - NetID: mvp307
++
+ package simplepb
+ 
+ //
+@@ -7,9 +11,11 @@ package simplepb
+ //
+ 
+ import (
+-	"sync"
+-
++	"container/heap"
+ 	"labrpc"
++	"log"
++	"sync"
++	"time"
+ )
+ 
+ // the 3 possible server status
+@@ -32,9 +38,13 @@ type PBServer struct {
+ 	commitIndex int           // all log entries <= commitIndex are considered to have been committed.
+ 
+ 	// ... other state that you might need ...
++	opIndex               int             // The operation index in the log assigned to the most recently received request, initially 0.
++	uncommittedOperations OperationsQueue // a priority queue of the operations to be added to the log
++	timeLastCommit        time.Time       // The last time a backup updated its commit index
++	noCommitThreshold     time.Duration   // The amount of time until not hearing from the primary is grounds for recovery or a view change
+ }
+ 
+-// Prepare defines the arguments for the Prepare RPC
++// PrepareArgs defines the arguments for the Prepare RPC
+ // Note that all field names must start with a capital letter for an RPC args struct
+ type PrepareArgs struct {
+ 	View          int         // the primary's current view
+@@ -50,12 +60,13 @@ type PrepareReply struct {
+ 	Success bool // whether the Prepare request has been accepted or rejected
+ }
+ 
+-// RecoverArgs defined the arguments for the Recovery RPC
++// RecoveryArgs defines the arguments for the Recovery RPC
+ type RecoveryArgs struct {
+ 	View   int // the view that the backup would like to synchronize with
+ 	Server int // the server sending the Recovery RPC (for debugging)
+ }
+ 
++// RecoveryReply defines the reply for the Recovery RPC
+ type RecoveryReply struct {
+ 	View          int           // the view of the primary
+ 	Entries       []interface{} // the primary's log including entries replicated up to and including the view.
+@@ -63,21 +74,27 @@ type RecoveryReply struct {
+ 	Success       bool          // whether the Recovery request has been accepted or rejected
+ }
+ 
++// ViewChangeArgs defines the arguments for the ViewChange RPC
+ type ViewChangeArgs struct {
+ 	View int // the new view to be changed into
+ }
+ 
++// ViewChangeReply defines the reply for the ViewChange RPC
+ type ViewChangeReply struct {
+ 	LastNormalView int           // the latest view which had a NORMAL status at the server
+ 	Log            []interface{} // the log at the server
++	CommitIndex    int           // Last commit index
+ 	Success        bool          // whether the ViewChange request has been accepted/rejected
+ }
+ 
++// StartViewArgs defines the arguments for the StartView RPC
+ type StartViewArgs struct {
+-	View int           // the new view which has completed view-change
+-	Log  []interface{} // the log associated with the new new
++	View        int           // the new view which has completed view-change
++	Log         []interface{} // the log associated with the new new
++	CommitIndex int           // Last commit index
+ }
+ 
++// StartViewReply defines the reply for the StartView RPC
+ type StartViewReply struct {
+ }
+ 
+@@ -87,15 +104,15 @@ func GetPrimary(view int, nservers int) int {
+ 	return view % nservers
+ }
+ 
++// IsPrimary is a predicate that returns true if this server is the primary for its current view, false otherwise.
++func (srv *PBServer) IsPrimary() bool {
++	return GetPrimary(srv.currentView, len(srv.peers)) == srv.me
++}
++
+ // IsCommitted is called by tester to check whether an index position
+ // has been considered committed by this server
+ func (srv *PBServer) IsCommitted(index int) (committed bool) {
+-	srv.mu.Lock()
+-	defer srv.mu.Unlock()
+-	if srv.commitIndex >= index {
+-		return true
+-	}
+-	return false
++	return srv.commitIndex >= index
+ }
+ 
+ // ViewStatus is called by tester to find out the current view of this server
+@@ -103,6 +120,7 @@ func (srv *PBServer) IsCommitted(index int) (committed bool) {
+ func (srv *PBServer) ViewStatus() (currentView int, statusIsNormal bool) {
+ 	srv.mu.Lock()
+ 	defer srv.mu.Unlock()
++
+ 	return srv.currentView, srv.status == NORMAL
+ }
+ 
+@@ -130,12 +148,18 @@ func (srv *PBServer) Kill() {
+ // startingView is the initial view (set to be zero) that all servers start in
+ func Make(peers []*labrpc.ClientEnd, me int, startingView int) *PBServer {
+ 	srv := &PBServer{
+-		peers:          peers,
+-		me:             me,
+-		currentView:    startingView,
+-		lastNormalView: startingView,
+-		status:         NORMAL,
++		peers:                 peers,
++		me:                    me,
++		currentView:           startingView,
++		lastNormalView:        startingView,
++		status:                NORMAL,
++		uncommittedOperations: make(OperationsQueue, 0),
++		timeLastCommit:        time.Now(),
++		noCommitThreshold:     2 * time.Second,
+ 	}
++
++	heap.Init(&srv.uncommittedOperations)
++
+ 	// all servers' log are initialized with a dummy command at index 0
+ 	var v interface{}
+ 	srv.log = append(srv.log, v)
+@@ -144,7 +168,7 @@ func Make(peers []*labrpc.ClientEnd, me int, startingView int) *PBServer {
+ 	return srv
+ }
+ 
+-// Start() is invoked by tester on some replica server to replicate a
++// Start is invoked by tester on some replica server to replicate a
+ // command.  Only the primary should process this request by appending
+ // the command to its log and then return *immediately* (while the log is being replicated to backup servers).
+ // if this server isn't the primary, returns false.
+@@ -156,21 +180,38 @@ func Make(peers []*labrpc.ClientEnd, me int, startingView int) *PBServer {
+ // *if it's eventually committed*. The second return value is the current
+ // view. The third return value is true if this server believes it is
+ // the primary.
+-func (srv *PBServer) Start(command interface{}) (
+-	index int, view int, ok bool) {
+-	srv.mu.Lock()
+-	defer srv.mu.Unlock()
++func (srv *PBServer) Start(command interface{}) (index int, view int, ok bool) {
+ 	// do not process command if status is not NORMAL
+ 	// and if i am not the primary in the current view
+-	if srv.status != NORMAL {
+-		return -1, srv.currentView, false
+-	} else if GetPrimary(srv.currentView, len(srv.peers)) != srv.me {
++	srv.mu.Lock()
++	defer srv.mu.Unlock()
++
++	if !srv.isNormalPrimary() {
+ 		return -1, srv.currentView, false
+ 	}
+ 
+-	// Your code here
++	// Normally, we would check the client table to see if we have already serviced this request.
++	// However, we do not have a last request number and cannot add one to the Start function.
++	srv.appendCommand(command)
++	go srv.primaryPrepare(srv.opIndex)
++
++	return srv.opIndex, srv.currentView, true
++}
++
++// assumes the mutex is already locked
++func (srv *PBServer) appendCommand(command interface{}) {
++	srv.opIndex++
++	srv.log = append(srv.log, command)
++}
++
++func (srv *PBServer) replicationFactor() int {
++	f := (len(srv.peers) - 1) / 2
++
++	if f <= 0 {
++		log.Fatalf("The replication factor f for the PBServer cannot be less than 1. %v <= 0", f)
++	}
+ 
+-	return index, view, ok
++	return f
+ }
+ 
+ // exmple code to send an AppendEntries RPC to a server.
+@@ -195,21 +236,37 @@ func (srv *PBServer) sendPrepare(server int, args *PrepareArgs, reply *PrepareRe
+ 
+ // Prepare is the RPC handler for the Prepare RPC
+ func (srv *PBServer) Prepare(args *PrepareArgs, reply *PrepareReply) {
+-	// Your code here
++	srv.backupPrepare(args, reply)
++}
++
++func (srv *PBServer) sendRecovery(server int, args *RecoveryArgs, reply *RecoveryReply) bool {
++	ok := srv.peers[server].Call("PBServer.Recovery", args, reply)
++	return ok
+ }
+ 
+ // Recovery is the RPC handler for the Recovery RPC
+ func (srv *PBServer) Recovery(args *RecoveryArgs, reply *RecoveryReply) {
+-	// Your code here
++	reply.View = srv.currentView
++	reply.Success = srv.isNormalPrimary() && args.View <= srv.currentView
++
++	if reply.Success {
++		reply.PrimaryCommit = srv.commitIndex
++		reply.Entries = srv.log
++
++		go srv.primaryPrepare(srv.opIndex)
++	}
++
++	log.Printf("Node %d received recover request in view %d from %d", srv.me, srv.currentView, args.Server)
+ }
+ 
+-// Some external oracle prompts the primary of the newView to
++// PromptViewChange starts a view change. Some external oracle prompts the primary of the newView to
+ // switch to the newView.
+ // PromptViewChange just kicks start the view change protocol to move to the newView
+ // It does not block waiting for the view change process to complete.
+ func (srv *PBServer) PromptViewChange(newView int) {
+ 	srv.mu.Lock()
+ 	defer srv.mu.Unlock()
++
+ 	newPrimary := GetPrimary(newView, len(srv.peers))
+ 
+ 	if newPrimary != srv.me { //only primary of newView should do view change
+@@ -217,74 +274,130 @@ func (srv *PBServer) PromptViewChange(newView int) {
+ 	} else if newView <= srv.currentView {
+ 		return
+ 	}
+-	vcArgs := &ViewChangeArgs{
+-		View: newView,
+-	}
++
++	vcArgs := &ViewChangeArgs{View: newView}
+ 	vcReplyChan := make(chan *ViewChangeReply, len(srv.peers))
+-	// send ViewChange to all servers including myself
+-	for i := 0; i < len(srv.peers); i++ {
+-		go func(server int) {
+-			var reply ViewChangeReply
+-			ok := srv.peers[server].Call("PBServer.ViewChange", vcArgs, &reply)
+-			// fmt.Printf("node-%d (nReplies %d) received reply ok=%v reply=%v\n", srv.me, nReplies, ok, r.reply)
+-			if ok {
+-				vcReplyChan <- &reply
+-			} else {
+-				vcReplyChan <- nil
+-			}
+-		}(i)
+-	}
++
++	srv.sendViewChange(vcArgs, vcReplyChan)
+ 
+ 	// wait to receive ViewChange replies
+ 	// if view change succeeds, send StartView RPC
+ 	go func() {
+-		var successReplies []*ViewChangeReply
+-		var nReplies int
+-		majority := len(srv.peers)/2 + 1
+-		for r := range vcReplyChan {
+-			nReplies++
+-			if r != nil && r.Success {
++		successReplies := make([]*ViewChangeReply, 0, len(srv.peers))
++		success := 0
++		failure := 0
++
++		for i := 1; i < len(srv.peers); i++ {
++			r := <-vcReplyChan
++
++			if r == nil {
++				failure++
++			} else {
++				success++
+ 				successReplies = append(successReplies, r)
+ 			}
+-			if nReplies == len(srv.peers) || len(successReplies) == majority {
+-				break
+-			}
+ 		}
+-		ok, log := srv.determineNewViewLog(successReplies)
++
++		log.Println("Received ViewChange replies from a majority of the replicas.")
++
++		ok, newLog, newCommitIndex := srv.determineNewViewLog(successReplies)
+ 		if !ok {
++			log.Printf("Unable to determine the log for the new view. Received %v replies, needed %v.", len(successReplies), srv.replicationFactor())
+ 			return
+ 		}
++
++		log.Printf("Determine %d as the log for the new view %d with commit index %d.", newLog, newView, newCommitIndex)
++
+ 		svArgs := &StartViewArgs{
+-			View: vcArgs.View,
+-			Log:  log,
++			View:        vcArgs.View,
++			Log:         newLog,
++			CommitIndex: srv.commitIndex,
+ 		}
++
+ 		// send StartView to all servers including myself
+ 		for i := 0; i < len(srv.peers); i++ {
+ 			var reply StartViewReply
+ 			go func(server int) {
+-				// fmt.Printf("node-%d sending StartView v=%d to node-%d\n", srv.me, svArgs.View, server)
++				log.Printf("node-%d sending StartView v=%d to node-%d\n", srv.me, svArgs.View, server)
+ 				srv.peers[server].Call("PBServer.StartView", svArgs, &reply)
+ 			}(i)
+ 		}
+ 	}()
+ }
+ 
++func (srv *PBServer) sendViewChange(args *ViewChangeArgs, replies chan *ViewChangeReply) {
++	// send ViewChange to all servers including myself
++	for i := 0; i < len(srv.peers); i++ {
++		go func(server int) {
++			reply := new(ViewChangeReply)
++			ok := srv.peers[server].Call("PBServer.ViewChange", args, reply)
++
++			if ok && reply.Success {
++				replies <- reply
++			} else {
++				replies <- nil
++			}
++		}(i)
++	}
++}
++
+ // determineNewViewLog is invoked to determine the log for the newView based on
+ // the collection of replies for successful ViewChange requests.
+ // if a quorum of successful replies exist, then ok is set to true.
+ // otherwise, ok = false.
+-func (srv *PBServer) determineNewViewLog(successReplies []*ViewChangeReply) (
+-	ok bool, newViewLog []interface{}) {
++func (srv *PBServer) determineNewViewLog(successReplies []*ViewChangeReply) (ok bool, newViewLog []interface{}, newCommitIndex int) {
+ 	// Your code here
+-	return ok, newViewLog
++	lastNormalView := -1
++	newCommitIndex = -1
++	newViewLog = make([]interface{}, 0)
++
++	// the new log is the one with the highest last normal view. If more than one such log exists, the longest log is used.
++	for i := range successReplies {
++		reply := successReplies[i]
++		if reply.Success && reply.LastNormalView >= lastNormalView && len(reply.Log) > len(newViewLog) {
++			newViewLog = reply.Log
++			lastNormalView = reply.LastNormalView
++
++			if reply.CommitIndex > newCommitIndex {
++				newCommitIndex = reply.CommitIndex
++			}
++		}
++	}
++
++	return len(successReplies) >= srv.replicationFactor(), newViewLog, newCommitIndex
+ }
+ 
+ // ViewChange is the RPC handler to process ViewChange RPC.
+ func (srv *PBServer) ViewChange(args *ViewChangeArgs, reply *ViewChangeReply) {
+-	// Your code here
++
++	reply.LastNormalView = srv.lastNormalView
++	reply.Log = srv.log
++	reply.CommitIndex = srv.commitIndex
++	reply.Success = args.View > srv.currentView
++
++	log.Printf("node-%d received ViewChange for view %d with status %v, and log %v.\n", srv.me, args.View, srv.status, srv.log)
++
++	if reply.Success {
++		srv.mu.Lock()
++		defer srv.mu.Unlock()
++		srv.status = VIEWCHANGE
++	}
+ }
+ 
+ // StartView is the RPC handler to process StartView RPC.
+ func (srv *PBServer) StartView(args *StartViewArgs, reply *StartViewReply) {
+ 	// Your code here
++	srv.mu.Lock()
++	defer srv.mu.Unlock()
++
++	log.Printf("node-%d received StartView for view %d.\n", srv.me, args.View)
++
++	if args.View > srv.currentView {
++		srv.currentView = args.View
++		srv.lastNormalView = args.View
++		srv.log = args.Log
++		srv.commitIndex = args.CommitIndex
++		srv.opIndex = len(args.Log) - 1
++		srv.status = NORMAL
++	}
+ }
+-- 
+2.11.0
+
diff --git a/src/simplepb/primary.go b/src/simplepb/primary.go
new file mode 100644
index 0000000..91b4c06
--- /dev/null
+++ b/src/simplepb/primary.go
@@ -0,0 +1,69 @@
+package simplepb
+
+import (
+	"log"
+)
+
+func (srv *PBServer) isNormalPrimary() bool {
+	return srv.status == NORMAL && srv.IsPrimary()
+}
+
+func (srv *PBServer) primaryPrepare(opIndex int) {
+	// Normally, we would update the client table with the new request number before sending Prepare messages.
+	arguments := &PrepareArgs{View: srv.currentView, PrimaryCommit: srv.commitIndex, Index: opIndex, Entry: srv.log[opIndex]}
+
+	log.Printf("Primary %v - Preparing (view: %v op: %v commit: %v entry: %v)", srv.me, srv.currentView, arguments.Index, srv.commitIndex, arguments.Entry)
+
+	replies := make(chan *PrepareReply, len(srv.peers))
+
+	for peer := range srv.peers {
+		if peer != srv.me {
+			go srv.primarySendPrepare(peer, arguments, replies)
+		}
+	}
+
+	go srv.primaryAwaitPrepare(arguments, replies)
+}
+
+func (srv *PBServer) primarySendPrepare(peer int, arguments *PrepareArgs, replies chan *PrepareReply) {
+	reply := new(PrepareReply)
+	completed := srv.sendPrepare(peer, arguments, reply)
+
+	if !completed {
+		replies <- nil
+	} else {
+		replies <- reply
+	}
+}
+
+// Awaits all prepare responses and timeouts.
+// Then, counts the number of successful replies. If >= f replies were successful, appends the next operation to the log.
+func (srv *PBServer) primaryAwaitPrepare(arguments *PrepareArgs, replies chan *PrepareReply) {
+	majority := srv.replicationFactor()
+	success := 0
+	failure := 0
+
+	// index starts at 1 in order to skip the primary.
+	// stops immediately after f successes
+	for i := 1; success < majority && i < len(srv.peers); i++ {
+		reply := <-replies
+
+		if reply == nil || !reply.Success {
+			failure++
+		} else {
+			success++
+		}
+	}
+
+	if success >= majority {
+		srv.mu.Lock()
+		defer srv.mu.Unlock()
+
+		if srv.commitIndex < arguments.Index && srv.isNormalPrimary() {
+			log.Printf("Primary %v - Updating commit %v -> %v", srv.me, srv.commitIndex, arguments.Index)
+			srv.commitIndex = arguments.Index
+		}
+	} else if !srv.isNormalPrimary() {
+		log.Printf("Primary %v - Failed serving operation %v", srv.me, arguments.Index)
+	}
+}
diff --git a/src/simplepb/priority_queue.go b/src/simplepb/priority_queue.go
new file mode 100644
index 0000000..d7a4200
--- /dev/null
+++ b/src/simplepb/priority_queue.go
@@ -0,0 +1,45 @@
+package simplepb
+
+// Prepare is a pair of input and output used by the backup
+type Prepare struct {
+	args  *PrepareArgs
+	reply *PrepareReply
+	done  chan bool
+}
+
+// OperationsQueue is a PriorityQueue that implements heap.Interface and holds *Prepare.
+type OperationsQueue []*Prepare
+
+// The length of the queue
+func (pq OperationsQueue) Len() int {
+	return len(pq)
+}
+
+// True if i is less than j
+func (pq OperationsQueue) Less(i, j int) bool {
+	return pq[i].args.Index < pq[j].args.Index
+}
+
+// Swap the positions of i and j
+func (pq OperationsQueue) Swap(i, j int) {
+	pq[i], pq[j] = pq[j], pq[i]
+}
+
+// Push x unto the queue
+func (pq *OperationsQueue) Push(x interface{}) {
+	*pq = append(*pq, x.(*Prepare))
+}
+
+// Pop the next operation to be committed from the queue
+func (pq *OperationsQueue) Pop() interface{} {
+	old := *pq
+	n := len(old)
+	item := old[n-1]
+	*pq = old[0 : n-1]
+	return item
+}
+
+// Peek at the next operation to be committed
+func (pq *OperationsQueue) Peek() *Prepare {
+	return (*pq)[0]
+}
diff --git a/src/simplepb/recover.go b/src/simplepb/recover.go
new file mode 100644
index 0000000..b7324b6
--- /dev/null
+++ b/src/simplepb/recover.go
@@ -0,0 +1,90 @@
+package simplepb
+
+import (
+	"log"
+	"time"
+)
+
+func (srv *PBServer) recover() {
+	arguments, replies := srv.startRecovery()
+	srv.awaitRecovery(arguments, replies)
+}
+
+func (srv *PBServer) startRecovery() (arguments *RecoveryArgs, replies chan *RecoveryReply) {
+	srv.mu.Lock()
+	defer srv.mu.Unlock()
+
+	if srv.status != NORMAL {
+		log.Printf("Node %v - not in normal status (view: %v op: %v commit: %v, status: %d)", srv.me, srv.currentView, srv.opIndex, srv.commitIndex, srv.status)
+		return
+	}
+
+	log.Printf("Node %v - Recovering (view: %v op: %v commit: %v)", srv.me, srv.currentView, srv.opIndex, srv.commitIndex)
+
+	srv.status = RECOVERING
+
+	arguments = &RecoveryArgs{View: srv.currentView, Server: srv.me}
+	replies = make(chan *RecoveryReply, len(srv.peers))
+
+	// Send recovery requests to all peers. However, only the primary will reply with success
+	for peer := range srv.peers {
+		if peer != srv.me {
+			go srv.sendRecoveryToPeer(peer, arguments, replies)
+		}
+	}
+
+	return arguments, replies
+}
+
+func (srv *PBServer) sendRecoveryToPeer(peer int, arguments *RecoveryArgs, replies chan *RecoveryReply) {
+	reply := new(RecoveryReply)
+	completed := srv.sendRecovery(peer, arguments, reply)
+
+	if !completed {
+		replies <- nil
+	} else {
+		replies <- reply
+	}
+}
+
+// Update log, op index, commit index and server status on receiving one recovery reply
+func (srv *PBServer) awaitRecovery(arguments *RecoveryArgs, replies chan *RecoveryReply) {
+	var primary *RecoveryReply
+	success := 0
+
+	// index starts at 1 in order to skip the current server.
+	for i := 1; i < len(srv.peers); i++ {
+		reply := <-replies
+
+		if reply != nil {
+			success++
+
+			if reply.Success {
+				primary = reply
+			}
+		}
+	}
+
+	if success >= srv.replicationFactor() && primary != nil {
+		srv.mu.Lock()
+		defer srv.mu.Unlock()
+
+		if primary.View == srv.currentView {
+			for i := len(srv.log); i < len(primary.Entries); i++ {
+				srv.appendCommand(primary.Entries[i])
+			}
+		} else {
+			srv.log = primary.Entries
+		}
+
+		srv.currentView = primary.View
+		srv.commitIndex = primary.PrimaryCommit
+		srv.timeLastCommit = time.Now()
+		srv.status = NORMAL
+		srv.lastNormalView = srv.currentView
+
+		log.Printf("Node %v - recovered with commit index %d and op index %d.\n", srv.me, srv.commitIndex, srv.opIndex)
+	} else {
+		log.Printf("Node %v - Did not received sufficient recovery replies (%d) or primary did not reply (%v)", srv.me, success, primary == nil)
+	}
+}
diff --git a/src/simplepb/server.go b/src/simplepb/server.go
index ca5efb4..9c6b11d 100644
--- a/src/simplepb/server.go
+++ b/src/simplepb/server.go
@@ -1,3 +1,7 @@
+// Authors:
+// Miguel David Salcedo   - NetID: mds796
+// Matheus Vieira Portela - NetID: mvp307
+
 package simplepb
 
 //
@@ -7,9 +11,11 @@ package simplepb
 //
 
 import (
-	"sync"
-
+	"container/heap"
 	"labrpc"
+	"log"
+	"sync"
+	"time"
 )
 
 // the 3 possible server status
@@ -32,9 +38,13 @@ type PBServer struct {
 	commitIndex int           // all log entries <= commitIndex are considered to have been committed.
 
 	// ... other state that you might need ...
+	opIndex               int             // The operation index in the log assigned to the most recently received request, initially 0.
+	uncommittedOperations OperationsQueue // a priority queue of the operations to be added to the log
+	timeLastCommit        time.Time       // The last time a backup updated its commit index
+	noCommitThreshold     time.Duration   // The amount of time until not hearing from the primary is grounds for recovery or a view change
 }
 
-// Prepare defines the arguments for the Prepare RPC
+// PrepareArgs defines the arguments for the Prepare RPC
 // Note that all field names must start with a capital letter for an RPC args struct
 type PrepareArgs struct {
 	View          int         // the primary's current view
@@ -50,12 +60,13 @@ type PrepareReply struct {
 	Success bool // whether the Prepare request has been accepted or rejected
 }
 
-// RecoverArgs defined the arguments for the Recovery RPC
+// RecoveryArgs defines the arguments for the Recovery RPC
 type RecoveryArgs struct {
 	View   int // the view that the backup would like to synchronize with
 	Server int // the server sending the Recovery RPC (for debugging)
 }
 
+// RecoveryReply defines the reply for the Recovery RPC
 type RecoveryReply struct {
 	View          int           // the view of the primary
 	Entries       []interface{} // the primary's log including entries replicated up to and including the view.
@@ -63,21 +74,27 @@ type RecoveryReply struct {
 	Success       bool          // whether the Recovery request has been accepted or rejected
 }
 
+// ViewChangeArgs defines the arguments for the ViewChange RPC
 type ViewChangeArgs struct {
 	View int // the new view to be changed into
 }
 
+// ViewChangeReply defines the reply for the ViewChange RPC
 type ViewChangeReply struct {
 	LastNormalView int           // the latest view which had a NORMAL status at the server
 	Log            []interface{} // the log at the server
+	CommitIndex    int           // Last commit index
 	Success        bool          // whether the ViewChange request has been accepted/rejected
 }
 
+// StartViewArgs defines the arguments for the StartView RPC
 type StartViewArgs struct {
-	View int           // the new view which has completed view-change
-	Log  []interface{} // the log associated with the new new
+	View        int           // the new view which has completed view-change
+	Log         []interface{} // the log associated with the new new
+	CommitIndex int           // Last commit index
 }
 
+// StartViewReply defines the reply for the StartView RPC
 type StartViewReply struct {
 }
 
@@ -87,15 +104,15 @@ func GetPrimary(view int, nservers int) int {
 	return view % nservers
 }
 
+// IsPrimary is a predicate that returns true if this server is the primary for its current view, false otherwise.
+func (srv *PBServer) IsPrimary() bool {
+	return GetPrimary(srv.currentView, len(srv.peers)) == srv.me
+}
+
 // IsCommitted is called by tester to check whether an index position
 // has been considered committed by this server
 func (srv *PBServer) IsCommitted(index int) (committed bool) {
-	srv.mu.Lock()
-	defer srv.mu.Unlock()
-	if srv.commitIndex >= index {
-		return true
-	}
-	return false
+	return srv.commitIndex >= index
 }
 
 // ViewStatus is called by tester to find out the current view of this server
@@ -103,6 +120,7 @@ func (srv *PBServer) IsCommitted(index int) (committed bool) {
 func (srv *PBServer) ViewStatus() (currentView int, statusIsNormal bool) {
 	srv.mu.Lock()
 	defer srv.mu.Unlock()
+
 	return srv.currentView, srv.status == NORMAL
 }
 
@@ -130,12 +148,18 @@ func (srv *PBServer) Kill() {
 // startingView is the initial view (set to be zero) that all servers start in
 func Make(peers []*labrpc.ClientEnd, me int, startingView int) *PBServer {
 	srv := &PBServer{
-		peers:          peers,
-		me:             me,
-		currentView:    startingView,
-		lastNormalView: startingView,
-		status:         NORMAL,
+		peers:                 peers,
+		me:                    me,
+		currentView:           startingView,
+		lastNormalView:        startingView,
+		status:                NORMAL,
+		uncommittedOperations: make(OperationsQueue, 0),
+		timeLastCommit:        time.Now(),
+		noCommitThreshold:     2 * time.Second,
 	}
+
+	heap.Init(&srv.uncommittedOperations)
+
 	// all servers' log are initialized with a dummy command at index 0
 	var v interface{}
 	srv.log = append(srv.log, v)
@@ -144,7 +168,7 @@ func Make(peers []*labrpc.ClientEnd, me int, startingView int) *PBServer {
 	return srv
 }
 
-// Start() is invoked by tester on some replica server to replicate a
+// Start is invoked by tester on some replica server to replicate a
 // command.  Only the primary should process this request by appending
 // the command to its log and then return *immediately* (while the log is being replicated to backup servers).
 // if this server isn't the primary, returns false.
@@ -156,21 +180,38 @@ func Make(peers []*labrpc.ClientEnd, me int, startingView int) *PBServer {
 // *if it's eventually committed*. The second return value is the current
 // view. The third return value is true if this server believes it is
 // the primary.
-func (srv *PBServer) Start(command interface{}) (
-	index int, view int, ok bool) {
-	srv.mu.Lock()
-	defer srv.mu.Unlock()
+func (srv *PBServer) Start(command interface{}) (index int, view int, ok bool) {
 	// do not process command if status is not NORMAL
 	// and if i am not the primary in the current view
-	if srv.status != NORMAL {
-		return -1, srv.currentView, false
-	} else if GetPrimary(srv.currentView, len(srv.peers)) != srv.me {
+	srv.mu.Lock()
+	defer srv.mu.Unlock()
+
+	if !srv.isNormalPrimary() {
 		return -1, srv.currentView, false
 	}
 
-	// Your code here
+	// Normally, we would check the client table to see if we have already serviced this request.
+	// However, we do not have a last request number and cannot add one to the Start function.
+	srv.appendCommand(command)
+	go srv.primaryPrepare(srv.opIndex)
+
+	return srv.opIndex, srv.currentView, true
+}
+
+// assumes the mutex is already locked
+func (srv *PBServer) appendCommand(command interface{}) {
+	srv.opIndex++
+	srv.log = append(srv.log, command)
+}
+
+func (srv *PBServer) replicationFactor() int {
+	f := (len(srv.peers) - 1) / 2
+
+	if f <= 0 {
+		log.Fatalf("The replication factor f for the PBServer cannot be less than 1. %v <= 0", f)
+	}
 
-	return index, view, ok
+	return f
 }
 
 // exmple code to send an AppendEntries RPC to a server.
@@ -195,21 +236,37 @@ func (srv *PBServer) sendPrepare(server int, args *PrepareArgs, reply *PrepareRe
 
 // Prepare is the RPC handler for the Prepare RPC
 func (srv *PBServer) Prepare(args *PrepareArgs, reply *PrepareReply) {
-	// Your code here
+	srv.backupPrepare(args, reply)
+}
+
+func (srv *PBServer) sendRecovery(server int, args *RecoveryArgs, reply *RecoveryReply) bool {
+	ok := srv.peers[server].Call("PBServer.Recovery", args, reply)
+	return ok
 }
 
 // Recovery is the RPC handler for the Recovery RPC
 func (srv *PBServer) Recovery(args *RecoveryArgs, reply *RecoveryReply) {
-	// Your code here
+	reply.View = srv.currentView
+	reply.Success = srv.isNormalPrimary() && args.View <= srv.currentView
+
+	if reply.Success {
+		reply.PrimaryCommit = srv.commitIndex
+		reply.Entries = srv.log
+
+		go srv.primaryPrepare(srv.opIndex)
+	}
+
+	log.Printf("Node %d received recover request in view %d from %d", srv.me, srv.currentView, args.Server)
 }
 
-// Some external oracle prompts the primary of the newView to
+// PromptViewChange starts a view change. Some external oracle prompts the primary of the newView to
 // switch to the newView.
 // PromptViewChange just kicks start the view change protocol to move to the newView
 // It does not block waiting for the view change process to complete.
 func (srv *PBServer) PromptViewChange(newView int) {
 	srv.mu.Lock()
 	defer srv.mu.Unlock()
+
 	newPrimary := GetPrimary(newView, len(srv.peers))
 
 	if newPrimary != srv.me { //only primary of newView should do view change
@@ -217,74 +274,130 @@ func (srv *PBServer) PromptViewChange(newView int) {
 	} else if newView <= srv.currentView {
 		return
 	}
-	vcArgs := &ViewChangeArgs{
-		View: newView,
-	}
+
+	vcArgs := &ViewChangeArgs{View: newView}
 	vcReplyChan := make(chan *ViewChangeReply, len(srv.peers))
-	// send ViewChange to all servers including myself
-	for i := 0; i < len(srv.peers); i++ {
-		go func(server int) {
-			var reply ViewChangeReply
-			ok := srv.peers[server].Call("PBServer.ViewChange", vcArgs, &reply)
-			// fmt.Printf("node-%d (nReplies %d) received reply ok=%v reply=%v\n", srv.me, nReplies, ok, r.reply)
-			if ok {
-				vcReplyChan <- &reply
-			} else {
-				vcReplyChan <- nil
-			}
-		}(i)
-	}
+
+	srv.sendViewChange(vcArgs, vcReplyChan)
 
 	// wait to receive ViewChange replies
 	// if view change succeeds, send StartView RPC
 	go func() {
-		var successReplies []*ViewChangeReply
-		var nReplies int
-		majority := len(srv.peers)/2 + 1
-		for r := range vcReplyChan {
-			nReplies++
-			if r != nil && r.Success {
+		successReplies := make([]*ViewChangeReply, 0, len(srv.peers))
+		success := 0
+		failure := 0
+
+		for i := 1; i < len(srv.peers); i++ {
+			r := <-vcReplyChan
+
+			if r == nil {
+				failure++
+			} else {
+				success++
 				successReplies = append(successReplies, r)
 			}
-			if nReplies == len(srv.peers) || len(successReplies) == majority {
-				break
-			}
 		}
-		ok, log := srv.determineNewViewLog(successReplies)
+
+		log.Println("Received ViewChange replies from a majority of the replicas.")
+
+		ok, newLog, newCommitIndex := srv.determineNewViewLog(successReplies)
 		if !ok {
+			log.Printf("Unable to determine the log for the new view. Received %v replies, needed %v.", len(successReplies), srv.replicationFactor())
 			return
 		}
+
+		log.Printf("Determine %d as the log for the new view %d with commit index %d.", newLog, newView, newCommitIndex)
+
 		svArgs := &StartViewArgs{
-			View: vcArgs.View,
-			Log:  log,
+			View:        vcArgs.View,
+			Log:         newLog,
+			CommitIndex: srv.commitIndex,
 		}
+
 		// send StartView to all servers including myself
 		for i := 0; i < len(srv.peers); i++ {
 			var reply StartViewReply
 			go func(server int) {
-				// fmt.Printf("node-%d sending StartView v=%d to node-%d\n", srv.me, svArgs.View, server)
+				log.Printf("node-%d sending StartView v=%d to node-%d\n", srv.me, svArgs.View, server)
 				srv.peers[server].Call("PBServer.StartView", svArgs, &reply)
 			}(i)
 		}
 	}()
 }
 
+func (srv *PBServer) sendViewChange(args *ViewChangeArgs, replies chan *ViewChangeReply) {
+	// send ViewChange to all servers including myself
+	for i := 0; i < len(srv.peers); i++ {
+		go func(server int) {
+			reply := new(ViewChangeReply)
+			ok := srv.peers[server].Call("PBServer.ViewChange", args, reply)
+
+			if ok && reply.Success {
+				replies <- reply
+			} else {
+				replies <- nil
+			}
+		}(i)
+	}
+}
+
 // determineNewViewLog is invoked to determine the log for the newView based on
 // the collection of replies for successful ViewChange requests.
 // if a quorum of successful replies exist, then ok is set to true.
 // otherwise, ok = false.
-func (srv *PBServer) determineNewViewLog(successReplies []*ViewChangeReply) (
-	ok bool, newViewLog []interface{}) {
+func (srv *PBServer) determineNewViewLog(successReplies []*ViewChangeReply) (ok bool, newViewLog []interface{}, newCommitIndex int) {
 	// Your code here
-	return ok, newViewLog
+	lastNormalView := -1
+	newCommitIndex = -1
+	newViewLog = make([]interface{}, 0)
+
+	// the new log is the one with the highest last normal view. If more than one such log exists, the longest log is used.
+	for i := range successReplies {
+		reply := successReplies[i]
+		if reply.Success && reply.LastNormalView >= lastNormalView && len(reply.Log) > len(newViewLog) {
+			newViewLog = reply.Log
+			lastNormalView = reply.LastNormalView
+
+			if reply.CommitIndex > newCommitIndex {
+				newCommitIndex = reply.CommitIndex
+			}
+		}
+	}
+
+	return len(successReplies) >= srv.replicationFactor(), newViewLog, newCommitIndex
 }
 
 // ViewChange is the RPC handler to process ViewChange RPC.
 func (srv *PBServer) ViewChange(args *ViewChangeArgs, reply *ViewChangeReply) {
-	// Your code here
+
+	reply.LastNormalView = srv.lastNormalView
+	reply.Log = srv.log
+	reply.CommitIndex = srv.commitIndex
+	reply.Success = args.View > srv.currentView
+
+	log.Printf("node-%d received ViewChange for view %d with status %v, and log %v.\n", srv.me, args.View, srv.status, srv.log)
+
+	if reply.Success {
+		srv.mu.Lock()
+		defer srv.mu.Unlock()
+		srv.status = VIEWCHANGE
+	}
 }
 
 // StartView is the RPC handler to process StartView RPC.
 func (srv *PBServer) StartView(args *StartViewArgs, reply *StartViewReply) {
 	// Your code here
+	srv.mu.Lock()
+	defer srv.mu.Unlock()
+
+	log.Printf("node-%d received StartView for view %d.\n", srv.me, args.View)
+
+	if args.View > srv.currentView {
+		srv.currentView = args.View
+		srv.lastNormalView = args.View
+		srv.log = args.Log
+		srv.commitIndex = args.CommitIndex
+		srv.opIndex = len(args.Log) - 1
+		srv.status = NORMAL
+	}
 }
-- 
2.11.0

